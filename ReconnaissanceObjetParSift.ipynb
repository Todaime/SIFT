{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies générales\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afficher texte sur image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_str(dst,x, y, s):\n",
    "    cv2.putText(dst, s,(x+1,y+1), cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 0),\n",
    "                thickness=2, lineType=cv2.LINE_AA)\n",
    "    cv2.putText(dst, s,(x,y), cv2.FONT_HERSHEY_PLAIN, 1.0, (255, 255, 255),\n",
    "                lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Descripteur_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Descripteur_Image\n",
    "# Pour une image, contient les informations necessaires à la reconnaissance de l'objet\n",
    "class Descripteur_Image(object):\n",
    "    \n",
    "    def __init__(self, nom_fichier, shape, image_couleur, points_cles, descripteurs):\n",
    "        \n",
    "        # Nom du fichier\n",
    "        self.nom_fichier = nom_fichier\n",
    "        \n",
    "        # Format de l'image\n",
    "        self.shape = shape\n",
    "        \n",
    "        # Forme couleur de l'image\n",
    "        self.image_couleur = image_couleur\n",
    "        \n",
    "        # Les points clés de l'image\n",
    "        self.points_cles = points_cles\n",
    "        \n",
    "        # Descripteurs des points clés\n",
    "        self.descripteurs = descripteurs\n",
    "        \n",
    "         # Association de l'image avec l'image de la video\n",
    "        self.reconnaissanceVideo = []\n",
    "        \n",
    "        # Association de la vidéo avec l'objet de la base de données\n",
    "        self.reconnaissanceBaseDonnee = []\n",
    "        \n",
    "    # Nettoyage des associations  effectuée pour un nouveau calcul\n",
    "    def nettoyageAssociation(self):\n",
    "        self.reconnaissanceVideo = []\n",
    "        self.reconnaissanceBaseDonnee = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des points depuis la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour chaque images de la base de donnée on charge les points clés\n",
    "def chargementPointsClesBaseDeDonnee():\n",
    "    # Retourne une liste d'\"ImageFeature\" contenant les descripteurs des Points clés\n",
    "    # des images de la base de donnée\n",
    "    \n",
    "    res = list()\n",
    "    \n",
    "    # On choisis le nombre de points clés par image (influe sur la fluidité de l'algorithme)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures=300)\n",
    "    \n",
    "    for image in os.listdir(\"BASE_DONNEES\"):\n",
    "        \n",
    "        # Chargement des images par openCV\n",
    "        image_couleur = cv2.imread(\"BASE_DONNEES/\" + str(image))\n",
    "        \n",
    "        # On change l'image couleur en image de niveau de gris\n",
    "        image_actuelle = cv2.cvtColor(image_couleur, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Obtention des points clés et des descripteurs de l'image\n",
    "        points_cles, descripteurs = sift.detectAndCompute(image_actuelle, None)\n",
    "        \n",
    "        # Features SIFT\n",
    "        res.append(Descripteur_Image(image, image_actuelle.shape, image_couleur, points_cles, descripteurs))\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associations entres les images de la BD et l'image video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul les associations de l'image video, avec les images de la base de données.\n",
    "# Parametre d'entreée : \n",
    "# - Données des images de la BD \n",
    "# - Descripteurs de l'image video\n",
    "# - Points clés de l'image video\n",
    "def trouverAssociation(data, descripteurs, points_cles):\n",
    "    \n",
    "    for img in data:\n",
    "        img.nettoyageAssociation()\n",
    "        for i in range(len(descripteurs)):\n",
    "            \n",
    "            # Norme de la difference entre le descripteur avec tous les autres descripteurs.\n",
    "            distanceAvecVideo = np.linalg.norm(descripteurs[i] - img.descripteurs, axis=-1)\n",
    "            \n",
    "            # Descripteur ayant une distance minimale par rapport au descripteur étudié\n",
    "            pointCandidat = distanceAvecVideo.argmin() \n",
    "            \n",
    "            # L'association est elle bidirectionnelle ?\n",
    "            distanceAvecBD = np.linalg.norm(img.descripteurs[pointCandidat] - descripteurs,axis=-1)\n",
    "            candidatImageVideo = distanceAvecBD.argmin()\n",
    "                \n",
    "            # Si l'association est bonne on fait l'affectation\n",
    "            if (i == candidatImageVideo):\n",
    "                img.reconnaissanceVideo.append(points_cles[i].pt)\n",
    "                img.reconnaissanceBaseDonnee.append(img.points_cles[pointCandidat].pt)\n",
    "                \n",
    "        # Conversion en tableau numpy\n",
    "        img.reconnaissanceVideo = np.array(img.reconnaissanceVideo)\n",
    "        img.reconnaissanceBaseDonnee = np.array(img.reconnaissanceBaseDonnee)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouver l'image de la base de donnée qui correspond le mieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul l'image ayant une meilleure correspondance selon la taille minimale du cluster\n",
    "def calculerMeilleureAssociation(data, nb_Erreur_Projection, minInliers):\n",
    "\n",
    "    bestIndex = None\n",
    "    bestMask = None\n",
    "    numInliers = 0\n",
    "    # Poour chaque image de la BD\n",
    "    for index, imgWithMatching in enumerate(data):\n",
    "        # donne la perspective entre deux plans, et evalue le nombre d'erreur par la methode RANSAC\n",
    "        _, mask = cv2.findHomography(imgWithMatching.reconnaissanceBaseDonnee, \n",
    "                                     imgWithMatching.reconnaissanceVideo, cv2.RANSAC, nb_Erreur_Projection)\n",
    "        if not mask is None:\n",
    "            # On verifie que la taille est à la fois plus grande que le minimum requis \n",
    "            # et que l'image precedente.\n",
    "            countNonZero = np.count_nonzero(mask)\n",
    "            if (countNonZero >= minInliers and countNonZero > numInliers):\n",
    "                numInliers = countNonZero\n",
    "                bestIndex = index\n",
    "                bestMask = (mask >= 1).reshape(-1)\n",
    "    # Si une image valide les criteres, on retourne celle-ci avec le cluster qui l'a validé\n",
    "    if not bestIndex is None:\n",
    "        bestImage = data[bestIndex]\n",
    "        inliersWebCam = bestImage.reconnaissanceVideo[bestMask]\n",
    "        inliersDataBase = bestImage.reconnaissanceBaseDonnee[bestMask]\n",
    "        return bestImage, inliersWebCam, inliersDataBase\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de correspondance + rectangle encerclant l'objet + affichage de l'objet correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule la matrice d'affinité, on affiche un rectangle entourant l'objet \n",
    "# et on ouvre une fenetre montrant l'objet reconnu.\n",
    "\n",
    "def matriceAffinite_et_dessin(image_meilleure_correspondance, clustersBaseDonnees, clustersVideo, image_sortie):\n",
    "    \n",
    "    # Matrice affinités\n",
    "    # transformation affine optimale entre des ensembles de points 2D\n",
    "    A = cv2.estimateRigidTransform(clustersBaseDonnees, clustersVideo, fullAffine=True)\n",
    "    A = np.vstack((A, [0, 0, 1]))\n",
    "    \n",
    "    # Points du rectangle entourant l'objet\n",
    "    a = np.array([0, 0, 1], np.float)\n",
    "    b = np.array([image_meilleure_correspondance.shape[1], 0, 1], np.float)\n",
    "    c = np.array([image_meilleure_correspondance.shape[1], image_meilleure_correspondance.shape[0], 1], np.float)\n",
    "    d = np.array([0, image_meilleure_correspondance.shape[0], 1], np.float)\n",
    "    centre = np.array([float(image_meilleure_correspondance.shape[0])/2, float(image_meilleure_correspondance.shape[1])/2, 1], np.float)\n",
    "       \n",
    "    # Mise à l'echelle des points pour fitter avec l'echelle de l'image video\n",
    "    a = np.dot(A, a)\n",
    "    b = np.dot(A, b)\n",
    "    c = np.dot(A, c)\n",
    "    d = np.dot(A, d)\n",
    "    centre = np.dot(A, centre)\n",
    "    \n",
    "    a_video = (int(a[0]/a[2]), int(a[1]/b[2]))\n",
    "    b_video = (int(b[0]/b[2]), int(b[1]/b[2]))\n",
    "    c_video = (int(c[0]/c[2]), int(c[1]/c[2]))\n",
    "    d_video = (int(d[0]/d[2]), int(d[1]/d[2]))\n",
    "    centre_video_x = int(centre[0]/centre[2])\n",
    "    centre_video_y = int(centre[1]/centre[2])\n",
    "    \n",
    "    # Affichage du rectangle\n",
    "    points = np.array([a_video, b_video, c_video, d_video], np.int32)\n",
    "    cv2.polylines(image_sortie, np.int32([points]),1, (255,255,255), thickness=3)\n",
    "    draw_str(image_sortie, centre_video_x,centre_video_y, image_meilleure_correspondance.nom_fichier.upper())\n",
    "    #Se visualiza el objeto detectado en una ventana a parte\n",
    "    cv2.imshow('Objet', image_meilleure_correspondance.image_couleur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Affichage de la fenetre du programme\n",
    "    def fenetre(*arg):\n",
    "        pass\n",
    "    cv2.namedWindow(\"Reconnaissance d'objets\")\n",
    "    cv2.namedWindow('Objet')\n",
    "    \n",
    "\n",
    "    # Taille cluster minimal pour reconnaitre l'objet\n",
    "    cv2.createTrackbar('Taille Cluster', \"Reconnaissance d'objets\", 20, 50, fenetre)\n",
    "    \n",
    "    # Choix de l'affichage des points clé\n",
    "    cv2.createTrackbar('Afficher Points cles', \"Reconnaissance d'objets\", 0,1, fenetre)\n",
    "    \n",
    "    cv2.createTrackbar('nb Erreur projection', \"Reconnaissance d'objets\",5,10, fenetre)\n",
    "    \n",
    "    vc = cv2.VideoCapture(0)\n",
    "    pause = False\n",
    "\n",
    "    # Chargement des points pour chaque image du dossier \"BASE_DONNEES\"\n",
    "    points_BD = chargementPointsClesBaseDeDonnee()\n",
    "    \n",
    "    # Boucle d'execution:\n",
    "    while True:\n",
    "        \n",
    "        if not pause:\n",
    "            rval, affichage = vc.read()\n",
    "        if affichage is None:\n",
    "            print('End of video input')\n",
    "            break\n",
    "\n",
    "        # Initialisation du detecteur SIFT :\n",
    "        detecteur = cv2.xfeatures2d.SIFT_create(nfeatures=300)\n",
    "                \n",
    "        # Conversion image entrée en gris:\n",
    "        image_entree = cv2.cvtColor(affichage, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Image de sortie (pour ne pas modifier l'image d'entree)\n",
    "        image_sortie = affichage.copy()\n",
    "        \n",
    "        # Calcul du temps et des points clés + descripteurs:\n",
    "        t1 = time.time()\n",
    "        points_cles, descripteurs = detecteur.detectAndCompute(image_entree, None)\n",
    "        \n",
    "        if len(points_BD) > 0:\n",
    "            # Associations de points clés entre l'image vidéo et celles des images de la BD\n",
    "            images_avec_association = trouverAssociation(points_BD, descripteurs, points_cles)    \n",
    "            \n",
    "            # Obtention du parametre concernant la taille minimale du cluster\n",
    "            taille_minimale_cluster = int(cv2.getTrackbarPos('Taille Cluster', \"Reconnaissance d'objets\"))\n",
    "            \n",
    "            nb_Erreur_Projection = float(cv2.getTrackbarPos('nb Erreur projection', \"Reconnaissance d'objets\"))\n",
    "            \n",
    "            # Calcul de l'image de BD correspondant au mieux \n",
    "            image_meilleure_correspondance, cluster_video, cluster_bd =  calculerMeilleureAssociation(points_BD, nb_Erreur_Projection,taille_minimale_cluster)            \n",
    "            \n",
    "            if not image_meilleure_correspondance is None:\n",
    "                # Une image correspond ->  Affichage sur l'image video de l'objet\n",
    "                matriceAffinite_et_dessin(image_meilleure_correspondance, cluster_bd, cluster_video, image_sortie)\n",
    "               \n",
    "        t1 = 1000 * (time.time() - t1)  # Temps en milisecondes\n",
    "        \n",
    "        # Taille des descripteurs des points clés:\n",
    "        if descripteurs is not None:\n",
    "            if len(descripteurs) > 0:\n",
    "                dim = len(descripteurs[0])\n",
    "            else:\n",
    "                dim = -1\n",
    "                \n",
    "        # Affichage des descripteurs et du texte de l'objet\n",
    "        if (int(cv2.getTrackbarPos('Afficher Points cles', \"Reconnaissance d'objets\")) > 0):\n",
    "            cv2.drawKeypoints(image_sortie, points_cles, image_sortie,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        draw_str(image_sortie,20, 20,\"Method {0}, {1} Points cles trouves, desc. dim. = {2} \".format('SIFT', len(points_cles), dim))\n",
    "        draw_str(image_sortie,20, 40, \"Temps calcul (ms): {0}\".format(str(t1)))\n",
    "        \n",
    "        # Affichage des resultats :\n",
    "        cv2.imshow(\"Reconnaissance d'objets\", image_sortie)\n",
    "        ch = cv2.waitKey(5) & 0xFF\n",
    "        if ch == ord(' '):  # Barre espace\n",
    "            pause = not pause\n",
    "\n",
    "    # Fermeture:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code inspiré de : https://gitlab.com/josemariasoladuran/object-recognition-opencv-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
